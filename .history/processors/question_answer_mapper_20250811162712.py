"""
Question Answer Mapper - Module mapping c√¢u h·ªèi v·ªõi l·ªùi gi·∫£i/ƒë√°p √°n
"""
import os
import re
from datetime import datetime
from config.vertex_ai_config import VertexAIConfig
from vertexai.generative_models import GenerativeModel, Part, GenerationConfig

class QuestionAnswerMapper:
    """Class x·ª≠ l√Ω mapping c√¢u h·ªèi v·ªõi ƒë√°p √°n/l·ªùi gi·∫£i"""
    
    def __init__(self):
        self.vertex_config = VertexAIConfig()
        self.model = None
        self._initialize_model()
    
    def _initialize_model(self):
        """Kh·ªüi t·∫°o Vertex AI model"""
        try:
            if self.vertex_config.is_configured():
                self.vertex_config.initialize_vertex_ai()
                self.model = GenerativeModel(
                    model_name="gemini-2.5-flash",  # S·ª≠ d·ª•ng Flash model cho mapping nhanh h∆°n
                    generation_config=GenerationConfig(
                        temperature=0.1,  # Th·∫•p ƒë·ªÉ ƒë·∫£m b·∫£o mapping ch√≠nh x√°c
                        top_p=0.8,
                        top_k=20,
                        max_output_tokens=32768  # TƒÉng l√™n 32K ƒë·ªÉ x·ª≠ l√Ω mapping ƒë·∫ßy ƒë·ªß
                    )
                )
                print("‚úÖ Vertex AI model ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o cho Question-Answer Mapper")
            else:
                print("‚ùå Vertex AI ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng")
        except Exception as e:
            print(f"‚ùå L·ªói kh·ªüi t·∫°o Vertex AI model: {e}")
    
    def extract_questions_and_answers(self, questions_content, answers_content):
        """
        Tr√≠ch xu·∫•t v√† ph√¢n t√≠ch c√¢u h·ªèi v√† ƒë√°p √°n t·ª´ n·ªôi dung
        Args:
            questions_content: N·ªôi dung ch·ª©a c√¢u h·ªèi
            answers_content: N·ªôi dung ch·ª©a ƒë√°p √°n/l·ªùi gi·∫£i
        Returns:
            dict: {'questions': [], 'answers': []}
        """
        questions = self._extract_questions(questions_content)
        answers = self._extract_answers(answers_content)
        
        return {
            'questions': questions,
            'answers': answers
        }
    
    def _extract_questions(self, content):
        """Tr√≠ch xu·∫•t danh s√°ch c√¢u h·ªèi"""
        questions = []
        
        # Pattern ƒë·ªÉ t√¨m c√¢u h·ªèi (linh ho·∫°t v·ªõi nhi·ªÅu format)
        question_patterns = [
            r'(?:^|\n)\s*(?:\*\*)?(?:C√¢u|Question)\s*(\d+)(?:\*\*)?[:\.]?\s*(.+?)(?=(?:^|\n)\s*(?:\*\*)?(?:C√¢u|Question)\s*\d+|\Z)',
            r'(?:^|\n)\s*(\d+)[:\.\)]\s*(.+?)(?=(?:^|\n)\s*\d+[:\.\)]|\Z)'
        ]
        
        for pattern in question_patterns:
            matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL | re.IGNORECASE)
            for match in matches:
                question_num = match.group(1)
                question_text = match.group(2).strip()
                
                if question_text and len(question_text) > 10:  # Filter out very short matches
                    questions.append({
                        'number': int(question_num),
                        'text': question_text,
                        'raw_match': match.group(0)
                    })
            
            if questions:  # N·∫øu ƒë√£ t√¨m th·∫•y c√¢u h·ªèi th√¨ d·ª´ng
                break
        
        # S·∫Øp x·∫øp theo s·ªë th·ª© t·ª±
        questions.sort(key=lambda x: x['number'])
        return questions
    
    def _extract_answers(self, content):
        """Tr√≠ch xu·∫•t danh s√°ch ƒë√°p √°n/l·ªùi gi·∫£i"""
        answers = []
        
        # Pattern ƒë·ªÉ t√¨m ƒë√°p √°n (linh ho·∫°t v·ªõi nhi·ªÅu format)
        answer_patterns = [
            r'(?:^|\n)\s*(?:\*\*)?(?:C√¢u|Question)\s*(\d+)(?:\*\*)?[:\.]?\s*(.+?)(?=(?:^|\n)\s*(?:\*\*)?(?:C√¢u|Question)\s*\d+|\Z)',
            r'(?:^|\n)\s*(\d+)[:\.\)]\s*(.+?)(?=(?:^|\n)\s*\d+[:\.\)]|\Z)',
            r'(?:^|\n)\s*(?:ƒê√°p √°n|Answer)\s*(?:c√¢u\s*)?(\d+)[:\.]?\s*(.+?)(?=(?:^|\n)\s*(?:ƒê√°p √°n|Answer)|\Z)'
        ]
        
        for pattern in answer_patterns:
            matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL | re.IGNORECASE)
            for match in matches:
                answer_num = match.group(1)
                answer_text = match.group(2).strip()
                
                if answer_text and len(answer_text) > 5:  # Filter out very short matches
                    answers.append({
                        'number': int(answer_num),
                        'text': answer_text,
                        'raw_match': match.group(0)
                    })
            
            if answers:  # N·∫øu ƒë√£ t√¨m th·∫•y ƒë√°p √°n th√¨ d·ª´ng
                break
        
        # S·∫Øp x·∫øp theo s·ªë th·ª© t·ª±
        answers.sort(key=lambda x: x['number'])
        return answers
    
    def map_questions_with_answers_ai(self, questions, answers):
        """
        S·ª≠ d·ª•ng AI ƒë·ªÉ mapping c√¢u h·ªèi v·ªõi ƒë√°p √°n
        Args:
            questions: List c√¢u h·ªèi
            answers: List ƒë√°p √°n
        Returns:
            List c√°c c·∫∑p question-answer ƒë√£ ƒë∆∞·ª£c mapping
        """
        if not self.model:
            print("‚ùå Model ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
            return []
        
        try:
            # T·∫°o prompt cho AI
            prompt = self._create_mapping_prompt(questions, answers)
            
            print(f"ü§ñ ƒêang mapping {len(questions)} c√¢u h·ªèi v·ªõi {len(answers)} ƒë√°p √°n...")
            
            response = self.model.generate_content(prompt)
            
            if response and response.text:
                mapped_pairs = self._parse_ai_mapping_response(response.text, questions, answers)
                print(f"‚úÖ ƒê√£ mapping th√†nh c√¥ng {len(mapped_pairs)} c·∫∑p c√¢u h·ªèi-ƒë√°p √°n")
                return mapped_pairs
            else:
                print("‚ùå AI kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£ mapping")
                return []
                
        except Exception as e:
            print(f"‚ùå L·ªói khi mapping b·∫±ng AI: {e}")
            return []
    
    def _create_mapping_prompt(self, questions, answers):
        """T·∫°o prompt cho AI ƒë·ªÉ mapping"""
        prompt = """
B·∫°n l√† m·ªôt chuy√™n gia gi√°o d·ª•c. Nhi·ªám v·ª• c·ªßa b·∫°n l√† mapping (gh√©p) c√¢u h·ªèi v·ªõi ƒë√°p √°n/l·ªùi gi·∫£i t∆∞∆°ng ·ª©ng.

DANH S√ÅCH C√ÇU H·ªéI:
"""
        
        for q in questions[:10]:  # Gi·ªõi h·∫°n 10 c√¢u ƒë·ªÉ tr√°nh prompt qu√° d√†i
            prompt += f"\n--- C√¢u {q['number']} ---\n{q['text'][:500]}...\n"
        
        prompt += "\n\nDANH S√ÅCH ƒê√ÅP √ÅN/L·ªúI GI·∫¢I:\n"
        
        for a in answers[:10]:  # Gi·ªõi h·∫°n 10 ƒë√°p √°n
            prompt += f"\n--- ƒê√°p √°n {a['number']} ---\n{a['text'][:500]}...\n"
        
        prompt += """

NHI·ªÜM V·ª§:
1. Ph√¢n t√≠ch n·ªôi dung t·ª´ng c√¢u h·ªèi v√† ƒë√°p √°n
2. X√°c ƒë·ªãnh m·ªëi li√™n h·ªá gi·ªØa c√¢u h·ªèi v√† ƒë√°p √°n d·ª±a tr√™n:
   - S·ªë th·ª© t·ª± (n·∫øu t∆∞∆°ng ·ª©ng)
   - N·ªôi dung ki·∫øn th·ª©c (m√¥n h·ªçc, ch·ªß ƒë·ªÅ)
   - T·ª´ kh√≥a chung
   - C·∫•u tr√∫c c√¢u h·ªèi-ƒë√°p √°n

ƒê·ªäNH D·∫†NG OUTPUT (JSON):
```json
{
  "mappings": [
    {
      "question_number": 1,
      "answer_number": 1,
      "confidence": 0.95,
      "reason": "L√Ω do mapping"
    }
  ]
}
```

H√£y mapping ch√≠nh x√°c v√† ghi r√µ l√Ω do cho m·ªói c·∫∑p:
"""
        
        return prompt
    
    def _parse_ai_mapping_response(self, response_text, questions, answers):
        """Parse k·∫øt qu·∫£ mapping t·ª´ AI"""
        mapped_pairs = []
        
        try:
            # T√¨m JSON trong response
            import json
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                mapping_data = json.loads(json_match.group(1))
                
                for mapping in mapping_data.get('mappings', []):
                    q_num = mapping['question_number']
                    a_num = mapping['answer_number']
                    
                    # T√¨m c√¢u h·ªèi v√† ƒë√°p √°n t∆∞∆°ng ·ª©ng
                    question = next((q for q in questions if q['number'] == q_num), None)
                    answer = next((a for a in answers if a['number'] == a_num), None)
                    
                    if question and answer:
                        mapped_pairs.append({
                            'question': question,
                            'answer': answer,
                            'confidence': mapping.get('confidence', 0.0),
                            'reason': mapping.get('reason', '')
                        })
            
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói parse AI response, fallback to simple mapping: {e}")
            # Fallback: Simple number-based mapping
            mapped_pairs = self._simple_number_mapping(questions, answers)
        
        return mapped_pairs
    
    def _simple_number_mapping(self, questions, answers):
        """Mapping ƒë∆°n gi·∫£n d·ª±a tr√™n s·ªë th·ª© t·ª±"""
        mapped_pairs = []
        
        for question in questions:
            # T√¨m ƒë√°p √°n c√≥ c√πng s·ªë th·ª© t·ª±
            answer = next((a for a in answers if a['number'] == question['number']), None)
            if answer:
                mapped_pairs.append({
                    'question': question,
                    'answer': answer,
                    'confidence': 0.8,
                    'reason': 'Number-based mapping'
                })
        
        return mapped_pairs
    
    def generate_mapped_content(self, mapped_pairs, output_format='markdown'):
        """
        T·∫°o n·ªôi dung ƒë√£ mapping
        Args:
            mapped_pairs: List c√°c c·∫∑p ƒë√£ mapping
            output_format: 'markdown' ho·∫∑c 'json'
        Returns:
            str: N·ªôi dung formatted
        """
        if output_format == 'markdown':
            return self._generate_markdown_output(mapped_pairs)
        elif output_format == 'json':
            return self._generate_json_output(mapped_pairs)
        else:
            return self._generate_markdown_output(mapped_pairs)
    
    def _generate_markdown_output(self, mapped_pairs):
        """T·∫°o output Markdown"""
        content = f"# ƒê·ªÅ thi ƒë√£ mapping c√¢u h·ªèi v√† l·ªùi gi·∫£i\n\n"
        content += f"*ƒê∆∞·ª£c t·∫°o t·ª± ƒë·ªông v√†o {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}*\n\n"
        content += f"**T·ªïng s·ªë c√¢u ƒë√£ mapping: {len(mapped_pairs)}**\n\n"
        content += "---\n\n"
        
        for i, pair in enumerate(mapped_pairs, 1):
            question = pair['question']
            answer = pair['answer']
            confidence = pair['confidence']
            
            content += f"## C√¢u {question['number']}\n\n"
            content += f"### üìù ƒê·ªÅ b√†i\n{question['text']}\n\n"
            content += f"### ‚úÖ L·ªùi gi·∫£i\n{answer['text']}\n\n"
            content += f"*ƒê·ªô tin c·∫≠y mapping: {confidence:.2f} - {pair['reason']}*\n\n"
            content += "---\n\n"
        
        return content
    
    def _generate_json_output(self, mapped_pairs):
        """T·∫°o output JSON"""
        import json
        
        output_data = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'total_pairs': len(mapped_pairs)
            },
            'mapped_questions': []
        }
        
        for pair in mapped_pairs:
            output_data['mapped_questions'].append({
                'question_number': pair['question']['number'],
                'question_text': pair['question']['text'],
                'answer_number': pair['answer']['number'],
                'answer_text': pair['answer']['text'],
                'mapping_confidence': pair['confidence'],
                'mapping_reason': pair['reason']
            })
        
        return json.dumps(output_data, ensure_ascii=False, indent=2)
    
    def process_files(self, questions_file, answers_file, output_file=None):
        """
        X·ª≠ l√Ω mapping t·ª´ 2 file v√† t·∫°o output
        Args:
            questions_file: File ch·ª©a c√¢u h·ªèi
            answers_file: File ch·ª©a ƒë√°p √°n
            output_file: File output (auto generate n·∫øu None)
        Returns:
            str: Path c·ªßa file output
        """
        try:
            # ƒê·ªçc n·ªôi dung files
            with open(questions_file, 'r', encoding='utf-8') as f:
                questions_content = f.read()
            
            with open(answers_file, 'r', encoding='utf-8') as f:
                answers_content = f.read()
            
            print(f"üìñ ƒê√£ ƒë·ªçc file c√¢u h·ªèi: {os.path.basename(questions_file)}")
            print(f"üìñ ƒê√£ ƒë·ªçc file ƒë√°p √°n: {os.path.basename(answers_file)}")
            
            # Tr√≠ch xu·∫•t c√¢u h·ªèi v√† ƒë√°p √°n
            extracted = self.extract_questions_and_answers(questions_content, answers_content)
            questions = extracted['questions']
            answers = extracted['answers']
            
            print(f"üîç T√¨m th·∫•y {len(questions)} c√¢u h·ªèi v√† {len(answers)} ƒë√°p √°n")
            
            if not questions or not answers:
                print("‚ùå Kh√¥ng t√¨m th·∫•y c√¢u h·ªèi ho·∫∑c ƒë√°p √°n")
                return None
            
            # Mapping b·∫±ng AI
            mapped_pairs = self.map_questions_with_answers_ai(questions, answers)
            
            if not mapped_pairs:
                print("‚ùå Kh√¥ng th·ªÉ mapping c√¢u h·ªèi v·ªõi ƒë√°p √°n")
                return None
            
            # T·∫°o output
            output_content = self.generate_mapped_content(mapped_pairs)
            
            # T·∫°o t√™n file output n·∫øu ch∆∞a c√≥
            if not output_file:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_file = f"data/output/mapped_questions_answers_{timestamp}.md"
            
            # ƒê·∫£m b·∫£o th∆∞ m·ª•c output t·ªìn t·∫°i
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            # Ghi file
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(output_content)
            
            print(f"‚úÖ ƒê√£ t·∫°o file mapping: {output_file}")
            return output_file
            
        except Exception as e:
            print(f"‚ùå L·ªói x·ª≠ l√Ω files: {e}")
            return None
    
    def process_single_file(self, input_file, output_file=None):
        """
        X·ª≠ l√Ω mapping t·ª´ 1 file .md ƒë∆°n l·∫ª (ch·ª©a c·∫£ c√¢u h·ªèi v√† ƒë√°p √°n)
        Args:
            input_file: File .md ch·ª©a c·∫£ c√¢u h·ªèi v√† ƒë√°p √°n/l·ªùi gi·∫£i
            output_file: File output (auto generate n·∫øu None)
        Returns:
            str: Path c·ªßa file output ho·∫∑c None n·∫øu l·ªói
        """
        try:
            # ƒê·ªçc n·ªôi dung file
            with open(input_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            print(f"üìñ ƒê√£ ƒë·ªçc file: {os.path.basename(input_file)}")
            
            if not self.model:
                print("‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o AI model cho mapping")
                return None
            
            # G·ª≠i to√†n b·ªô n·ªôi dung cho AI ƒë·ªÉ x·ª≠ l√Ω mapping
            print("ü§ñ ƒêang g·ª≠i n·ªôi dung cho AI ƒë·ªÉ mapping...")
            mapped_content = self._process_content_with_ai(content)
            
            if not mapped_content:
                print("‚ùå AI kh√¥ng th·ªÉ x·ª≠ l√Ω mapping")
                return None
            
            # T·∫°o t√™n file output n·∫øu ch∆∞a c√≥
            if not output_file:
                base_name = os.path.splitext(os.path.basename(input_file))[0]
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_file = f"data/output/{base_name}_mapped_{timestamp}.md"
            
            # ƒê·∫£m b·∫£o th∆∞ m·ª•c output t·ªìn t·∫°i
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            # Ghi file
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(mapped_content)
            
            print(f"‚úÖ ƒê√£ t·∫°o file mapping: {output_file}")
            return output_file
            
        except Exception as e:
            print(f"‚ùå L·ªói x·ª≠ l√Ω file: {e}")
            return None
    
    def _process_content_with_ai(self, content):
        """
        G·ª≠i to√†n b·ªô n·ªôi dung cho AI ƒë·ªÉ x·ª≠ l√Ω mapping
        Args:
            content: N·ªôi dung ƒë·∫ßy ƒë·ªß t·ª´ file .md
        Returns:
            str: N·ªôi dung ƒë√£ ƒë∆∞·ª£c AI mapping ho·∫∑c None n·∫øu l·ªói
        """
        try:
            # Ki·ªÉm tra ƒë·ªô d√†i content ƒë·ªÉ quy·∫øt ƒë·ªãnh c√°ch x·ª≠ l√Ω
            content_length = len(content)
            print(f"üìè ƒê·ªô d√†i n·ªôi dung: {content_length:,} k√Ω t·ª±")
            
            # N·∫øu content qu√° l·ªõn (>50k k√Ω t·ª±), chia nh·ªè ƒë·ªÉ x·ª≠ l√Ω
            if content_length > 50000:
                print("‚ö†Ô∏è N·ªôi dung l·ªõn, s·∫Ω chia nh·ªè ƒë·ªÉ x·ª≠ l√Ω...")
                return self._process_large_content_in_chunks(content)
            
            # T·∫°o prompt th√¥ng minh h∆°n cho AI
            prompt = f"""
**## Nhi·ªám v·ª•:** Mapping c√¢u h·ªèi v·ªõi l·ªùi gi·∫£i t·ª´ ƒë·ªÅ thi ho√†n ch·ªânh

**## C·∫•u tr√∫c ƒë·ªÅ thi ƒë·∫ßu v√†o:**
1. **Ph·∫ßn c√¢u h·ªèi:** C√°c c√¢u tr·∫Øc nghi·ªám, ƒë√∫ng/sai, t·ª± lu·∫≠n v·ªõi format `**C√¢u X:**`
2. **Ph·∫ßn ƒë√°p √°n:** C√≥ th·ªÉ l√†:
   - B·∫£ng ƒë√°p √°n tr·∫Øc nghi·ªám (A, B, C, D)
   - L·ªùi gi·∫£i chi ti·∫øt cho c√¢u t·ª± lu·∫≠n
   - H∆∞·ªõng d·∫´n gi·∫£i t·ª´ng b∆∞·ªõc

**## Quy t·∫Øc mapping:**
1. **C√¢u tr·∫Øc nghi·ªám:** T√¨m ƒë√°p √°n trong b·∫£ng ƒë√°p √°n v√† t·∫°o l·ªùi gi·∫£i ng·∫Øn
2. **C√¢u t·ª± lu·∫≠n:** T√¨m l·ªùi gi·∫£i chi ti·∫øt ƒë·∫ßy ƒë·ªß
3. **C√¢u ƒë√∫ng/sai:** T√¨m b·∫£ng ƒê√∫ng/Sai t∆∞∆°ng ·ª©ng

**## Format output:**
```
**C√¢u [S·ªë]:** [N·ªôi dung c√¢u h·ªèi ƒë·∫ßy ƒë·ªß v·ªõi c√°c l·ª±a ch·ªçn]
L·ªùi gi·∫£i
[ƒê√°p √°n + gi·∫£i th√≠ch ng·∫Øn g·ªçn]
```

**## V√≠ d·ª• cho c√¢u tr·∫Øc nghi·ªám:**
```
**C√¢u 1:** H√†m s·ªë y=f(x) ƒë·ªìng bi·∫øn tr√™n kho·∫£ng n√†o?
A. (-‚àû;-2)  B. (-2;1)  C. (-2;3)  D. (1;+‚àû)
L·ªùi gi·∫£i
ƒê√°p √°n: B. (-2;1)
T·ª´ b·∫£ng bi·∫øn thi√™n, f'(x) > 0 tr√™n kho·∫£ng (-2;1) n√™n h√†m s·ªë ƒë·ªìng bi·∫øn tr√™n kho·∫£ng n√†y.
```

**## H√£y x·ª≠ l√Ω to√†n b·ªô ƒë·ªÅ thi sau v√† mapping T·∫§T C·∫¢ c√°c c√¢u:**
{content}
"""
            
            response = self.model.generate_content(prompt)
            
            if response and response.text:
                print("‚úÖ AI ƒë√£ x·ª≠ l√Ω mapping th√†nh c√¥ng")
                return response.text
            else:
                print("‚ùå AI kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£")
                return None
                
        except Exception as e:
            print(f"‚ùå L·ªói khi g·ª≠i cho AI: {e}")
            return None
    
    def _process_large_content_in_chunks(self, content):
        """
        Chia content l·ªõn th√†nh c√°c chunk nh·ªè h∆°n ƒë·ªÉ x·ª≠ l√Ω
        Args:
            content: N·ªôi dung l·ªõn c·∫ßn chia nh·ªè
        Returns:
            str: N·ªôi dung ƒë√£ ƒë∆∞·ª£c mapping t·ª´ t·∫•t c·∫£ c√°c chunk
        """
        try:
            # Chia content theo c√¢u h·ªèi (pattern "C√¢u X:")
            question_pattern = r'(?=\*\*C√¢u\s+\d+(?:\.\d+)*[:\.]?\*\*)'
            chunks = re.split(question_pattern, content)
            
            # Lo·∫°i b·ªè chunk r·ªóng v√† gi·ªØ l·∫°i chunk ƒë·∫ßu n·∫øu c√≥ header
            chunks = [chunk.strip() for chunk in chunks if chunk.strip()]
            
            print(f"üì¶ Chia content th√†nh {len(chunks)} chunk(s)")
            
            processed_chunks = []
            
            for i, chunk in enumerate(chunks):
                chunk_length = len(chunk)
                print(f"üîÑ X·ª≠ l√Ω chunk {i+1}/{len(chunks)} ({chunk_length:,} k√Ω t·ª±)")
                
                # N·∫øu chunk v·∫´n qu√° l·ªõn, chia ti·∫øp
                if chunk_length > 40000:
                    # Chia theo ƒëo·∫°n vƒÉn ho·∫∑c c√¢u
                    sub_chunks = self._split_chunk_further(chunk)
                    for j, sub_chunk in enumerate(sub_chunks):
                        print(f"  üîÑ X·ª≠ l√Ω sub-chunk {j+1}/{len(sub_chunks)}")
                        result = self._process_single_chunk(sub_chunk)
                        if result:
                            processed_chunks.append(result)
                else:
                    result = self._process_single_chunk(chunk)
                    if result:
                        processed_chunks.append(result)
            
            # G·ªôp t·∫•t c·∫£ k·∫øt qu·∫£
            final_result = '\n\n'.join(processed_chunks)
            print(f"‚úÖ ƒê√£ x·ª≠ l√Ω xong {len(processed_chunks)} chunk(s)")
            
            return final_result
            
        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω content l·ªõn: {e}")
            return None
    
    def _split_chunk_further(self, chunk):
        """
        Chia chunk l·ªõn th√†nh c√°c sub-chunk nh·ªè h∆°n
        """
        # Chia theo paragraphs
        paragraphs = chunk.split('\n\n')
        sub_chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            if len(current_chunk + paragraph) < 35000:
                current_chunk += paragraph + '\n\n'
            else:
                if current_chunk:
                    sub_chunks.append(current_chunk.strip())
                current_chunk = paragraph + '\n\n'
        
        if current_chunk:
            sub_chunks.append(current_chunk.strip())
        
        return sub_chunks
    
    def _process_single_chunk(self, chunk):
        """
        X·ª≠ l√Ω m·ªôt chunk ƒë∆°n l·∫ª
        """
        try:
            prompt = f"""
H√£y √°nh x·∫° (mapping) c√¢u h·ªèi v·ªõi l·ªùi gi·∫£i trong n·ªôi dung sau v√† tr·∫£ v·ªÅ theo format:

**C√¢u [S·ªë]:** [N·ªôi dung c√¢u h·ªèi]
L·ªùi gi·∫£i
[N·ªôi dung l·ªùi gi·∫£i]

N·ªôi dung c·∫ßn x·ª≠ l√Ω:
{chunk}
"""
            
            response = self.model.generate_content(prompt)
            
            if response and response.text:
                return response.text
            else:
                print("‚ö†Ô∏è Chunk kh√¥ng ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng")
                return None
                
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω chunk: {e}")
            return None
    
    def process_content_with_ai(self, content):
        """
        Public method ƒë·ªÉ g·ª≠i n·ªôi dung cho AI x·ª≠ l√Ω mapping
        Args:
            content: N·ªôi dung c·∫ßn mapping
        Returns:
            str: N·ªôi dung ƒë√£ mapping ho·∫∑c None
        """
        return self._process_content_with_ai(content)
    
    def _split_content_for_mapping(self, content):
        """
        T√°ch n·ªôi dung th√†nh 2 ph·∫ßn: c√¢u h·ªèi v√† l·ªùi gi·∫£i
        Args:
            content: N·ªôi dung ƒë·∫ßy ƒë·ªß t·ª´ file .md
        Returns:
            tuple: (questions_content, answers_content)
        """
        # C√°ch 1: T√¨m ph·∫ßn "ƒë√°p √°n" ho·∫∑c "l·ªùi gi·∫£i"
        content_lower = content.lower()
        
        # T√¨m c√°c t·ª´ kh√≥a ph√¢n t√°ch
        split_keywords = [
            'ƒë√°p √°n', 'l·ªùi gi·∫£i', 'h∆∞·ªõng d·∫´n gi·∫£i', 'gi·∫£i chi ti·∫øt',
            'answer', 'solution', 'explanation', 'detailed solution',
            'key answer', 'answer key', 'solutions'
        ]
        
        split_pos = -1
        split_keyword = ""
        
        for keyword in split_keywords:
            pos = content_lower.find(keyword)
            if pos != -1:
                split_pos = pos
                split_keyword = keyword
                print(f"üîç T√¨m th·∫•y t·ª´ kh√≥a ph√¢n t√°ch: '{keyword}' t·∫°i v·ªã tr√≠ {pos}")
                break
        
        if split_pos != -1:
            # T√°ch t·∫°i v·ªã tr√≠ t√¨m th·∫•y
            questions_content = content[:split_pos].strip()
            answers_content = content[split_pos:].strip()
            print(f"‚úÖ ƒê√£ t√°ch n·ªôi dung d·ª±a tr√™n t·ª´ kh√≥a '{split_keyword}'")
            return questions_content, answers_content
        
        # C√°ch 2: T√¨m theo pattern ### ho·∫∑c ## ho·∫∑c ---
        lines = content.split('\n')
        questions_lines = []
        answers_lines = []
        
        current_section = 'questions'
        section_changed = False
        
        for i, line in enumerate(lines):
            line_lower = line.lower().strip()
            
            # Ki·ªÉm tra c√°c pattern ph√¢n t√°ch section
            if (line.startswith('---') or 
                (line.startswith('#') and any(keyword in line_lower for keyword in split_keywords)) or
                any(keyword in line_lower for keyword in split_keywords)):
                if not section_changed:
                    current_section = 'answers'
                    section_changed = True
                    print(f"üîç Chuy·ªÉn sang ph·∫ßn ƒë√°p √°n t·∫°i d√≤ng {i+1}: '{line[:50]}...'")
            
            if current_section == 'questions':
                questions_lines.append(line)
            else:
                answers_lines.append(line)
        
        questions_content = '\n'.join(questions_lines).strip()
        answers_content = '\n'.join(answers_lines).strip()
        
        # C√°ch 3: N·∫øu v·∫´n kh√¥ng t√°ch ƒë∆∞·ª£c, t√¨m theo pattern c√¢u h·ªèi li·ªÅn k·ªÅ ƒë√°p √°n
        if not section_changed and questions_content == answers_content:
            # T√¨m pattern: C√¢u X + options + C√¢u Y (c√≥ th·ªÉ l√† ƒë√°p √°n)
            question_answer_pairs = self._detect_question_answer_pairs(content)
            if question_answer_pairs:
                questions_content = '\n\n'.join([pair['question'] for pair in question_answer_pairs])
                answers_content = '\n\n'.join([pair['answer'] for pair in question_answer_pairs if pair['answer']])
                print(f"‚úÖ ƒê√£ t√°ch {len(question_answer_pairs)} c·∫∑p c√¢u h·ªèi-ƒë√°p √°n")
                return questions_content, answers_content
        
        # N·∫øu kh√¥ng t√°ch ƒë∆∞·ª£c, tr·∫£ v·ªÅ to√†n b·ªô content cho c·∫£ 2 ph·∫ßn
        if not questions_content or not answers_content:
            print("‚ö†Ô∏è Kh√¥ng th·ªÉ t√°ch r√µ r√†ng, s·ª≠ d·ª•ng to√†n b·ªô n·ªôi dung cho c·∫£ c√¢u h·ªèi v√† ƒë√°p √°n")
            return content, content
        
        print(f"‚úÖ ƒê√£ t√°ch n·ªôi dung: {len(questions_lines)} d√≤ng c√¢u h·ªèi, {len(answers_lines)} d√≤ng ƒë√°p √°n")
        return questions_content, answers_content
    
    def _detect_question_answer_pairs(self, content):
        """
        Ph√°t hi·ªán c√°c c·∫∑p c√¢u h·ªèi-ƒë√°p √°n trong c√πng m·ªôt n·ªôi dung
        Returns:
            list: [{'question': str, 'answer': str}, ...]
        """
        pairs = []
        
        # Pattern ph·ª©c t·∫°p h∆°n ƒë·ªÉ t√¨m c√¢u h·ªèi + ƒë√°p √°n
        pattern = r'(?:^|\n)\s*(?:\*\*)?(?:C√¢u|Question)\s*(\d+)(?:\*\*)?[:\.]?\s*(.+?)(?=(?:^|\n)\s*(?:\*\*)?(?:C√¢u|Question)\s*\d+|\Z)'
        
        matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL | re.IGNORECASE)
        
        for match in matches:
            question_num = match.group(1)
            full_content = match.group(2).strip()
            
            # T√¨m xem trong n·ªôi dung n√†y c√≥ ƒë√°p √°n kh√¥ng
            answer_patterns = [
                r'(?:ƒë√°p √°n|answer)[:\s]*([A-D]\.?.*?)(?=\n|$)',
                r'(?:l·ªùi gi·∫£i|solution)[:\s]*(.+?)(?=\n\n|$)',
                r'\*\*([A-D])\*\*',
                r'^([A-D])[\.\)]\s*(.+?)$'
            ]
            
            question_text = full_content
            answer_text = ""
            
            for ans_pattern in answer_patterns:
                ans_match = re.search(ans_pattern, full_content, re.MULTILINE | re.IGNORECASE)
                if ans_match:
                    answer_text = ans_match.group(0)
                    question_text = full_content.replace(answer_text, '').strip()
                    break
            
            if question_text:
                pairs.append({
                    'question': f"C√¢u {question_num}: {question_text}",
                    'answer': f"C√¢u {question_num}: {answer_text}" if answer_text else ""
                })
        
        return pairs
